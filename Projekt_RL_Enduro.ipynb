{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importujemy biblioteki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#niezbędne biblioteki\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from skimage import data\n",
    "from skimage.color import rgb2gray\n",
    "from keras import backend as K\n",
    "from keras.models     import Sequential, Model\n",
    "from keras.layers     import Dense, Conv2D, Flatten, MaxPooling2D, Input, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import deque\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0b18d4b470>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('Enduro-v0')\n",
    "plt.imshow(rgb2gray(env.reset()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesing ramek\n",
    "Obcinamy część ramki bo zawiera nieistotne informacje informacje. Resztę przedstawiamy jako obrazek czarno biały i normalizujemy wartosci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    img=rgb2gray(frame)\n",
    "    img=img[50:155,:]\n",
    "    img=img/255\n",
    "    img=img.reshape(*img.shape, -1)\n",
    "    return img\n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "        frame = preprocess_frame(state)\n",
    "        if is_new_episode:\n",
    "            #Czyścimy stos\n",
    "            stacked_frames = deque([np.zeros((105,160), dtype=np.int) for i in range(4)], maxlen=4)\n",
    "            \n",
    "            #Na pocżatku nowej gry nie mamy stosu, więc go tworzymy\n",
    "            stacked_frames.append(frame)\n",
    "            stacked_frames.append(frame)\n",
    "            stacked_frames.append(frame)\n",
    "            stacked_frames.append(frame)\n",
    "\n",
    "            stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "        else:\n",
    "            # Append frame to deque, automatically removes the oldest frame\n",
    "            stacked_frames.append(frame)\n",
    "\n",
    "            # Build the stacked state (first dimension specifies different frames)\n",
    "            stacked_state = np.stack(stacked_frames, axis=2) \n",
    "\n",
    "        return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#piszemy klasę, która będzie odpowiadała za działania naszego agenta\n",
    "class DQNSuperAgent:\n",
    "    def __init__(self, state_size):\n",
    "        self.state_size = state_size\n",
    "        self.memory = deque(maxlen=5000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (5,5), strides=3, input_shape=(105, 160, 4), activation='elu'))\n",
    "        model.add(Conv2D(60, (4,4), strides=2, activation='elu'))\n",
    "        model.add(Conv2D(60, (3,3), activation='elu'))\n",
    "        model.add(MaxPooling2D())\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(150, activation='relu'))\n",
    "        model.add(Dense(4, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randint(0,3)\n",
    "        else:\n",
    "            state=state.reshape(1, *state.shape)\n",
    "            act_values = self.model.predict(state)\n",
    "            return np.argmax(act_values[0]) #Wybieramy akcję o największej przewidywanej nagrodzie\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = np.array(self.memory)[:batch_size]\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                next_state=next_state.reshape(1, *next_state.shape)\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0]) #to [0] bierze się stąd, że ramka \n",
    "                #z wynikami jest w innej ramce\n",
    "            state=state.reshape(1, *state.shape)\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#klasa z zaimplementowaną rywalizującą siecią neuronową\n",
    "class DuelingAgent:\n",
    "    def __init__(self, state_size):\n",
    "        self.state_size = state_size\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "    def _build_model(self):\n",
    "        # Uses the network architecture found in DeepMind paper\n",
    "        model = Sequential()\n",
    "        input_layer = Input(shape = (105, 160,1))\n",
    "        conv1 = Conv2D(32, (9,9), strides=3, padding='same', activation='relu')(input_layer)\n",
    "        max1 = MaxPooling2D((2,2))(conv1)\n",
    "        conv2 = Conv2D(64, (5, 5), strides=2, activation='relu')(max1)\n",
    "        conv3 = Conv2D(64, (3, 3), activation = 'relu')(conv2)\n",
    "        flatten = Flatten()(conv3)\n",
    "        fc1 = Dense(512)(flatten)\n",
    "        advantage = Dense(4)(fc1)\n",
    "        fc2 = Dense(512)(flatten)\n",
    "        value = Dense(1)(fc2)\n",
    "        policy = Lambda(lambda x: x[0]-K.mean(x[0])+x[1], output_shape = (4,))([advantage, value])\n",
    "        model = Model(input=[input_layer], output=[policy])\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randint(0,3)\n",
    "        else:\n",
    "            state=state.reshape(1, *state.shape)\n",
    "            act_values = self.model.predict(state)\n",
    "            return np.argmax(act_values[0]) #Wybieramy akcję o największej przewidywanej nagrodzie\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                next_state=next_state.reshape(1, *next_state.shape)\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0]) #to [0] bierze się stąd, że ramka \n",
    "                #z wynikami jest w innej ramce\n",
    "            state=state.reshape(1, *state.shape)\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arkadiusz/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1/400, score: 500.0, cars: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_3 to have 4 dimensions, but got array with shape (1, 105, 160, 4, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6368074ca209>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# train the agent with the experience of the episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-2c674cb98eb3>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mnext_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#to [0] bierze się stąd, że ramka\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                 \u001b[0;31m#z wynikami jest w innej ramce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_3 to have 4 dimensions, but got array with shape (1, 105, 160, 4, 1)"
     ]
    }
   ],
   "source": [
    "#obecnie niedziałająca realizacja z stackowaniem klatek\n",
    "if __name__ == \"__main__\":\n",
    "    # initialize gym environment and the agent\n",
    "    env = gym.make('Enduro-v0')\n",
    "    agent = DuelingAgent(env)\n",
    "    episodes=400\n",
    "    stack_size = 4 # We stack 4 frames\n",
    "    # Initialize deque with zero-images one array for each image\n",
    "    stacked_frames  =  deque([np.zeros((105,160), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
    "    # Iterate the game\n",
    "    for e in range(episodes):\n",
    "        score = 0\n",
    "        # reset state in the beginning of each game\n",
    "        state = env.reset()\n",
    "        cars=0\n",
    "        max_cars=0\n",
    "        for time_t in range(5000):\n",
    "            if time_t==0:\n",
    "                state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "                action = agent.act(state)\n",
    "                # Advance the game to the next frame based on the action.\n",
    "                # Reward is 1 for every frame the pole survived\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                if reward==1:\n",
    "                    cars+=1\n",
    "                    reward=10\n",
    "                    if cars>max_cars:\n",
    "                        max_cars=cars\n",
    "                        reward+=100\n",
    "                if reward==-1:\n",
    "                    cars-=1\n",
    "                    reward=-10\n",
    "                score=score+reward\n",
    "            # turn this on if you want to render\n",
    "            # env.render()\n",
    "            else:\n",
    "                action = agent.act(state)\n",
    "                # Advance the game to the next frame based on the action.\n",
    "                # Reward is 1 for every frame the pole survived\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                if reward==1:\n",
    "                    cars+=1\n",
    "                    reward=10\n",
    "                    if cars>max_cars:\n",
    "                        max_cars=cars\n",
    "                        reward+=100\n",
    "                if reward==-1:\n",
    "                    cars-=1\n",
    "                    reward=-10\n",
    "                score=score+reward\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                # Remember the previous state, action, reward, and done\n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "                # make next_state the new current state for the next frame.\n",
    "                state = next_state\n",
    "                # done becomes True when the game ends\n",
    "                # ex) The agent drops the pole\n",
    "            if done or time_t==4999:\n",
    "                # print the score and break out of the loop\n",
    "                print(\"episode: {}/{}, score: {}, cars: {}\"\n",
    "                      .format(e+1, episodes, score, cars))\n",
    "                break\n",
    "        # train the agent with the experience of the episode\n",
    "        agent.replay(1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arkadiusz/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1/400, score: 0.0, cars: 0\n",
      "episode: 2/400, score: 0.0, cars: 0\n",
      "episode: 3/400, score: 0.0, cars: 0\n",
      "episode: 4/400, score: 0.0, cars: 0\n",
      "episode: 5/400, score: 0.0, cars: 0\n",
      "episode: 6/400, score: 2.0, cars: 0\n",
      "episode: 7/400, score: 0.0, cars: 0\n",
      "episode: 8/400, score: 1.0, cars: 0\n",
      "episode: 9/400, score: 0.0, cars: 0\n",
      "episode: 10/400, score: 0.0, cars: 0\n",
      "episode: 11/400, score: 0.0, cars: 0\n",
      "episode: 12/400, score: 0.0, cars: 0\n",
      "episode: 13/400, score: 0.0, cars: 0\n",
      "episode: 14/400, score: 0.0, cars: 0\n",
      "episode: 15/400, score: 10.0, cars: 0\n",
      "episode: 16/400, score: 0.0, cars: 0\n",
      "episode: 17/400, score: 0.0, cars: 0\n",
      "episode: 18/400, score: 0.0, cars: 0\n",
      "episode: 19/400, score: 0.0, cars: 0\n",
      "episode: 20/400, score: 0.0, cars: 0\n",
      "episode: 21/400, score: 2.0, cars: 0\n",
      "episode: 22/400, score: 0.0, cars: 0\n",
      "episode: 23/400, score: 1.0, cars: 0\n",
      "episode: 24/400, score: 0.0, cars: 0\n",
      "episode: 25/400, score: 0.0, cars: 0\n",
      "episode: 26/400, score: 2.0, cars: 0\n",
      "episode: 27/400, score: 1.0, cars: 0\n",
      "episode: 28/400, score: 1.0, cars: 0\n",
      "episode: 29/400, score: 0.0, cars: 0\n",
      "episode: 30/400, score: 2.0, cars: 0\n",
      "episode: 31/400, score: 1.0, cars: 0\n",
      "episode: 32/400, score: 3.0, cars: 0\n",
      "episode: 33/400, score: 0.0, cars: 0\n",
      "episode: 34/400, score: 1.0, cars: 0\n",
      "episode: 35/400, score: 0.0, cars: 0\n",
      "episode: 36/400, score: 0.0, cars: 0\n",
      "episode: 37/400, score: 13.0, cars: 0\n",
      "episode: 38/400, score: 0.0, cars: 0\n",
      "episode: 39/400, score: 2.0, cars: 0\n",
      "episode: 40/400, score: 0.0, cars: 0\n",
      "episode: 41/400, score: 2.0, cars: 0\n",
      "episode: 42/400, score: 8.0, cars: 0\n",
      "episode: 43/400, score: 0.0, cars: 0\n",
      "episode: 44/400, score: 0.0, cars: 0\n",
      "episode: 45/400, score: 8.0, cars: 0\n",
      "episode: 46/400, score: 0.0, cars: 0\n",
      "episode: 47/400, score: 0.0, cars: 0\n",
      "episode: 48/400, score: 0.0, cars: 0\n",
      "episode: 49/400, score: 12.0, cars: 0\n",
      "episode: 50/400, score: 0.0, cars: 0\n",
      "episode: 51/400, score: 0.0, cars: 0\n",
      "episode: 52/400, score: 0.0, cars: 0\n",
      "episode: 53/400, score: 0.0, cars: 0\n",
      "episode: 54/400, score: 0.0, cars: 0\n",
      "episode: 55/400, score: 1.0, cars: 0\n",
      "episode: 56/400, score: 5.0, cars: 0\n",
      "episode: 57/400, score: 0.0, cars: 0\n",
      "episode: 58/400, score: 1.0, cars: 0\n",
      "episode: 59/400, score: 25.0, cars: 0\n",
      "episode: 60/400, score: 0.0, cars: 0\n",
      "episode: 61/400, score: 0.0, cars: 0\n",
      "episode: 62/400, score: 0.0, cars: 0\n",
      "episode: 63/400, score: 0.0, cars: 0\n",
      "episode: 64/400, score: 2.0, cars: 0\n",
      "episode: 65/400, score: 8.0, cars: 0\n",
      "episode: 66/400, score: 0.0, cars: 0\n",
      "episode: 67/400, score: 0.0, cars: 0\n",
      "episode: 68/400, score: 0.0, cars: 0\n",
      "episode: 69/400, score: 1.0, cars: 0\n",
      "episode: 70/400, score: 8.0, cars: 0\n",
      "episode: 71/400, score: 0.0, cars: 0\n",
      "episode: 72/400, score: 0.0, cars: 0\n",
      "episode: 73/400, score: 0.0, cars: 0\n",
      "episode: 74/400, score: 1.0, cars: 0\n",
      "episode: 75/400, score: 0.0, cars: 0\n",
      "episode: 76/400, score: 0.0, cars: 0\n",
      "episode: 77/400, score: 1.0, cars: 0\n",
      "episode: 78/400, score: 0.0, cars: 0\n",
      "episode: 79/400, score: 0.0, cars: 0\n",
      "episode: 80/400, score: 0.0, cars: 0\n",
      "episode: 81/400, score: 0.0, cars: 0\n",
      "episode: 82/400, score: 2.0, cars: 0\n",
      "episode: 83/400, score: 0.0, cars: 0\n",
      "episode: 84/400, score: 0.0, cars: 0\n",
      "episode: 85/400, score: 3.0, cars: 0\n",
      "episode: 86/400, score: 0.0, cars: 0\n",
      "episode: 87/400, score: 0.0, cars: 0\n",
      "episode: 88/400, score: 0.0, cars: 0\n",
      "episode: 89/400, score: 0.0, cars: 0\n",
      "episode: 90/400, score: 0.0, cars: 0\n",
      "episode: 91/400, score: 7.0, cars: 0\n",
      "episode: 92/400, score: 0.0, cars: 0\n",
      "episode: 93/400, score: 1.0, cars: 0\n",
      "episode: 94/400, score: 0.0, cars: 0\n",
      "episode: 95/400, score: 0.0, cars: 0\n",
      "episode: 96/400, score: 0.0, cars: 0\n",
      "episode: 97/400, score: 0.0, cars: 0\n",
      "episode: 98/400, score: 0.0, cars: 0\n",
      "episode: 99/400, score: 0.0, cars: 0\n",
      "episode: 100/400, score: 0.0, cars: 0\n",
      "episode: 101/400, score: 0.0, cars: 0\n",
      "episode: 102/400, score: 0.0, cars: 0\n",
      "episode: 103/400, score: 0.0, cars: 0\n",
      "episode: 104/400, score: 0.0, cars: 0\n",
      "episode: 105/400, score: 0.0, cars: 0\n",
      "episode: 106/400, score: 12.0, cars: 0\n",
      "episode: 107/400, score: 5.0, cars: 0\n",
      "episode: 108/400, score: 0.0, cars: 0\n",
      "episode: 109/400, score: 0.0, cars: 0\n",
      "episode: 110/400, score: 0.0, cars: 0\n",
      "episode: 111/400, score: 0.0, cars: 0\n",
      "episode: 112/400, score: 0.0, cars: 0\n",
      "episode: 113/400, score: 0.0, cars: 0\n",
      "episode: 114/400, score: 0.0, cars: 0\n",
      "episode: 115/400, score: 3.0, cars: 0\n",
      "episode: 116/400, score: 5.0, cars: 0\n",
      "episode: 117/400, score: 0.0, cars: 0\n",
      "episode: 118/400, score: 0.0, cars: 0\n",
      "episode: 119/400, score: 0.0, cars: 0\n",
      "episode: 120/400, score: 0.0, cars: 0\n",
      "episode: 121/400, score: 0.0, cars: 0\n",
      "episode: 122/400, score: 2.0, cars: 0\n",
      "episode: 123/400, score: 11.0, cars: 0\n",
      "episode: 124/400, score: 4.0, cars: 0\n",
      "episode: 125/400, score: 0.0, cars: 0\n",
      "episode: 126/400, score: 0.0, cars: 0\n",
      "episode: 127/400, score: 0.0, cars: 0\n",
      "episode: 128/400, score: 0.0, cars: 0\n",
      "episode: 129/400, score: 0.0, cars: 0\n",
      "episode: 130/400, score: 1.0, cars: 0\n",
      "episode: 131/400, score: 17.0, cars: 0\n",
      "episode: 132/400, score: 0.0, cars: 0\n",
      "episode: 133/400, score: 0.0, cars: 0\n",
      "episode: 134/400, score: 9.0, cars: 0\n",
      "episode: 135/400, score: 0.0, cars: 0\n",
      "episode: 136/400, score: 0.0, cars: 0\n",
      "episode: 137/400, score: 4.0, cars: 0\n",
      "episode: 138/400, score: 0.0, cars: 0\n",
      "episode: 139/400, score: 0.0, cars: 0\n",
      "episode: 140/400, score: 0.0, cars: 0\n",
      "episode: 141/400, score: 0.0, cars: 0\n",
      "episode: 142/400, score: 0.0, cars: 0\n",
      "episode: 143/400, score: 0.0, cars: 0\n",
      "episode: 144/400, score: 11.0, cars: 0\n",
      "episode: 145/400, score: 0.0, cars: 0\n",
      "episode: 146/400, score: 5.0, cars: 0\n",
      "episode: 147/400, score: 0.0, cars: 0\n",
      "episode: 148/400, score: 0.0, cars: 0\n",
      "episode: 149/400, score: 0.0, cars: 0\n",
      "episode: 150/400, score: 0.0, cars: 0\n",
      "episode: 151/400, score: 4.0, cars: 0\n",
      "episode: 152/400, score: 3.0, cars: 0\n",
      "episode: 153/400, score: 0.0, cars: 0\n",
      "episode: 154/400, score: 0.0, cars: 0\n",
      "episode: 155/400, score: 32.0, cars: 0\n",
      "episode: 156/400, score: 0.0, cars: 0\n",
      "episode: 157/400, score: 0.0, cars: 0\n",
      "episode: 158/400, score: 13.0, cars: 0\n",
      "episode: 159/400, score: 0.0, cars: 0\n",
      "episode: 160/400, score: 0.0, cars: 0\n",
      "episode: 161/400, score: 10.0, cars: 0\n",
      "episode: 162/400, score: 0.0, cars: 0\n",
      "episode: 163/400, score: 0.0, cars: 0\n",
      "episode: 164/400, score: 21.0, cars: 0\n",
      "episode: 165/400, score: 11.0, cars: 0\n",
      "episode: 166/400, score: 0.0, cars: 0\n",
      "episode: 167/400, score: 0.0, cars: 0\n",
      "episode: 168/400, score: 8.0, cars: 0\n",
      "episode: 169/400, score: 0.0, cars: 0\n",
      "episode: 170/400, score: 0.0, cars: 0\n",
      "episode: 171/400, score: 0.0, cars: 0\n",
      "episode: 172/400, score: 0.0, cars: 0\n",
      "episode: 173/400, score: 1.0, cars: 0\n",
      "episode: 174/400, score: 26.0, cars: 0\n",
      "episode: 175/400, score: 0.0, cars: 0\n",
      "episode: 176/400, score: 2.0, cars: 0\n",
      "episode: 177/400, score: 36.0, cars: 0\n",
      "episode: 178/400, score: 0.0, cars: 0\n",
      "episode: 179/400, score: 0.0, cars: 0\n",
      "episode: 180/400, score: 0.0, cars: 0\n",
      "episode: 181/400, score: 1.0, cars: 0\n",
      "episode: 182/400, score: 0.0, cars: 0\n",
      "episode: 183/400, score: 0.0, cars: 0\n",
      "episode: 184/400, score: 0.0, cars: 0\n",
      "episode: 185/400, score: 20.0, cars: 0\n",
      "episode: 186/400, score: 0.0, cars: 0\n",
      "episode: 187/400, score: 0.0, cars: 0\n",
      "episode: 188/400, score: 0.0, cars: 0\n",
      "episode: 189/400, score: 11.0, cars: 0\n",
      "episode: 190/400, score: 0.0, cars: 0\n",
      "episode: 191/400, score: 0.0, cars: 0\n",
      "episode: 192/400, score: 0.0, cars: 0\n",
      "episode: 193/400, score: 3.0, cars: 0\n",
      "episode: 194/400, score: 0.0, cars: 0\n",
      "episode: 195/400, score: 0.0, cars: 0\n",
      "episode: 196/400, score: 0.0, cars: 0\n",
      "episode: 197/400, score: 1.0, cars: 0\n",
      "episode: 198/400, score: 0.0, cars: 0\n",
      "episode: 199/400, score: 0.0, cars: 0\n",
      "episode: 200/400, score: 0.0, cars: 0\n",
      "episode: 201/400, score: 20.0, cars: 0\n",
      "episode: 202/400, score: 0.0, cars: 0\n",
      "episode: 203/400, score: 0.0, cars: 0\n",
      "episode: 204/400, score: 0.0, cars: 0\n",
      "episode: 205/400, score: 4.0, cars: 0\n",
      "episode: 206/400, score: 4.0, cars: 0\n",
      "episode: 207/400, score: 0.0, cars: 0\n",
      "episode: 208/400, score: 12.0, cars: 0\n",
      "episode: 209/400, score: 0.0, cars: 0\n",
      "episode: 210/400, score: 5.0, cars: 0\n",
      "episode: 211/400, score: 0.0, cars: 0\n",
      "episode: 212/400, score: 0.0, cars: 0\n",
      "episode: 213/400, score: 0.0, cars: 0\n",
      "episode: 214/400, score: 0.0, cars: 0\n",
      "episode: 215/400, score: 14.0, cars: 0\n",
      "episode: 216/400, score: 0.0, cars: 0\n",
      "episode: 217/400, score: 0.0, cars: 0\n",
      "episode: 218/400, score: 15.0, cars: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 219/400, score: 0.0, cars: 0\n",
      "episode: 220/400, score: 0.0, cars: 0\n",
      "episode: 221/400, score: 0.0, cars: 0\n",
      "episode: 222/400, score: 33.0, cars: 0\n",
      "episode: 223/400, score: 11.0, cars: 0\n",
      "episode: 224/400, score: 1.0, cars: 0\n",
      "episode: 225/400, score: 0.0, cars: 0\n",
      "episode: 226/400, score: 0.0, cars: 0\n",
      "episode: 227/400, score: 0.0, cars: 0\n",
      "episode: 228/400, score: 0.0, cars: 0\n",
      "episode: 229/400, score: 0.0, cars: 0\n",
      "episode: 230/400, score: 0.0, cars: 0\n",
      "episode: 231/400, score: 0.0, cars: 0\n",
      "episode: 232/400, score: 0.0, cars: 0\n",
      "episode: 233/400, score: 0.0, cars: 0\n",
      "episode: 234/400, score: 10.0, cars: 0\n",
      "episode: 235/400, score: 0.0, cars: 0\n",
      "episode: 236/400, score: 6.0, cars: 0\n",
      "episode: 237/400, score: 0.0, cars: 0\n",
      "episode: 238/400, score: 9.0, cars: 0\n",
      "episode: 239/400, score: 0.0, cars: 0\n",
      "episode: 240/400, score: 0.0, cars: 0\n",
      "episode: 241/400, score: 0.0, cars: 0\n",
      "episode: 242/400, score: 0.0, cars: 0\n",
      "episode: 243/400, score: 0.0, cars: 0\n",
      "episode: 244/400, score: 0.0, cars: 0\n",
      "episode: 245/400, score: 0.0, cars: 0\n",
      "episode: 246/400, score: 0.0, cars: 0\n",
      "episode: 247/400, score: 1.0, cars: 0\n",
      "episode: 248/400, score: 9.0, cars: 0\n",
      "episode: 249/400, score: 0.0, cars: 0\n",
      "episode: 250/400, score: 0.0, cars: 0\n",
      "episode: 251/400, score: 0.0, cars: 0\n",
      "episode: 252/400, score: 29.0, cars: 0\n",
      "episode: 253/400, score: 0.0, cars: 0\n",
      "episode: 254/400, score: 0.0, cars: 0\n",
      "episode: 255/400, score: 11.0, cars: 0\n",
      "episode: 256/400, score: 0.0, cars: 0\n",
      "episode: 257/400, score: 0.0, cars: 0\n",
      "episode: 258/400, score: 35.0, cars: 0\n",
      "episode: 259/400, score: 8.0, cars: 0\n",
      "episode: 260/400, score: 0.0, cars: 0\n",
      "episode: 261/400, score: 3.0, cars: 0\n",
      "episode: 262/400, score: 0.0, cars: 0\n",
      "episode: 263/400, score: 13.0, cars: 0\n",
      "episode: 264/400, score: 25.0, cars: 0\n",
      "episode: 265/400, score: 3.0, cars: 0\n",
      "episode: 266/400, score: 0.0, cars: 0\n",
      "episode: 267/400, score: 0.0, cars: 0\n",
      "episode: 268/400, score: 20.0, cars: 0\n",
      "episode: 269/400, score: 12.0, cars: 0\n",
      "episode: 270/400, score: 0.0, cars: 0\n",
      "episode: 271/400, score: 0.0, cars: 0\n",
      "episode: 272/400, score: 0.0, cars: 0\n",
      "episode: 273/400, score: 32.0, cars: 0\n",
      "episode: 274/400, score: 0.0, cars: 0\n",
      "episode: 275/400, score: 1.0, cars: 0\n",
      "episode: 276/400, score: 0.0, cars: 0\n",
      "episode: 277/400, score: 0.0, cars: 0\n",
      "episode: 278/400, score: 22.0, cars: 0\n",
      "episode: 279/400, score: 13.0, cars: 0\n",
      "episode: 280/400, score: 6.0, cars: 0\n",
      "episode: 281/400, score: 0.0, cars: 0\n",
      "episode: 282/400, score: 0.0, cars: 0\n",
      "episode: 283/400, score: 0.0, cars: 0\n",
      "episode: 284/400, score: 25.0, cars: 0\n",
      "episode: 285/400, score: 0.0, cars: 0\n",
      "episode: 286/400, score: 6.0, cars: 0\n",
      "episode: 287/400, score: 8.0, cars: 0\n",
      "episode: 288/400, score: 0.0, cars: 0\n",
      "episode: 289/400, score: 9.0, cars: 0\n",
      "episode: 290/400, score: 0.0, cars: 0\n",
      "episode: 291/400, score: 0.0, cars: 0\n",
      "episode: 292/400, score: 0.0, cars: 0\n",
      "episode: 293/400, score: 10.0, cars: 0\n",
      "episode: 294/400, score: 0.0, cars: 0\n",
      "episode: 295/400, score: 13.0, cars: 0\n",
      "episode: 296/400, score: 29.0, cars: 0\n",
      "episode: 297/400, score: 0.0, cars: 0\n",
      "episode: 298/400, score: 0.0, cars: 0\n",
      "episode: 299/400, score: 0.0, cars: 0\n",
      "episode: 300/400, score: 6.0, cars: 0\n",
      "episode: 301/400, score: 33.0, cars: 0\n",
      "episode: 302/400, score: 0.0, cars: 0\n",
      "episode: 303/400, score: 0.0, cars: 0\n",
      "episode: 304/400, score: 18.0, cars: 0\n",
      "episode: 305/400, score: 0.0, cars: 0\n",
      "episode: 306/400, score: 1.0, cars: 0\n",
      "episode: 307/400, score: 28.0, cars: 0\n",
      "episode: 308/400, score: 0.0, cars: 0\n",
      "episode: 309/400, score: 3.0, cars: 0\n",
      "episode: 310/400, score: 0.0, cars: 0\n",
      "episode: 311/400, score: 0.0, cars: 0\n",
      "episode: 312/400, score: 0.0, cars: 0\n",
      "episode: 313/400, score: 0.0, cars: 0\n",
      "episode: 314/400, score: 0.0, cars: 0\n",
      "episode: 315/400, score: 0.0, cars: 0\n",
      "episode: 316/400, score: 0.0, cars: 0\n",
      "episode: 317/400, score: 0.0, cars: 0\n",
      "episode: 318/400, score: 0.0, cars: 0\n",
      "episode: 319/400, score: 2.0, cars: 0\n",
      "episode: 320/400, score: 11.0, cars: 0\n",
      "episode: 321/400, score: 0.0, cars: 0\n",
      "episode: 322/400, score: 46.0, cars: 0\n",
      "episode: 323/400, score: 3.0, cars: 0\n",
      "episode: 324/400, score: 24.0, cars: 0\n",
      "episode: 325/400, score: 0.0, cars: 0\n",
      "episode: 326/400, score: 0.0, cars: 0\n",
      "episode: 327/400, score: 0.0, cars: 0\n",
      "episode: 328/400, score: 14.0, cars: 0\n",
      "episode: 329/400, score: 0.0, cars: 0\n",
      "episode: 330/400, score: 0.0, cars: 0\n",
      "episode: 331/400, score: 0.0, cars: 0\n",
      "episode: 332/400, score: 0.0, cars: 0\n",
      "episode: 333/400, score: 0.0, cars: 0\n",
      "episode: 334/400, score: 0.0, cars: 0\n",
      "episode: 335/400, score: 0.0, cars: 0\n",
      "episode: 336/400, score: 0.0, cars: 0\n",
      "episode: 337/400, score: 0.0, cars: 0\n",
      "episode: 338/400, score: 33.0, cars: 0\n",
      "episode: 339/400, score: 0.0, cars: 0\n",
      "episode: 340/400, score: 0.0, cars: 0\n",
      "episode: 341/400, score: 0.0, cars: 0\n",
      "episode: 342/400, score: 23.0, cars: 0\n",
      "episode: 343/400, score: 0.0, cars: 0\n",
      "episode: 344/400, score: 0.0, cars: 0\n",
      "episode: 345/400, score: 0.0, cars: 0\n",
      "episode: 346/400, score: 23.0, cars: 0\n",
      "episode: 347/400, score: 17.0, cars: 0\n",
      "episode: 348/400, score: 0.0, cars: 0\n",
      "episode: 349/400, score: 0.0, cars: 0\n",
      "episode: 350/400, score: 0.0, cars: 0\n",
      "episode: 351/400, score: 0.0, cars: 0\n",
      "episode: 352/400, score: 27.0, cars: 0\n",
      "episode: 353/400, score: 0.0, cars: 0\n",
      "episode: 354/400, score: 14.0, cars: 0\n",
      "episode: 355/400, score: 0.0, cars: 0\n",
      "episode: 356/400, score: 39.0, cars: 0\n",
      "episode: 357/400, score: 0.0, cars: 0\n",
      "episode: 358/400, score: 0.0, cars: 0\n",
      "episode: 359/400, score: 0.0, cars: 0\n",
      "episode: 360/400, score: 41.0, cars: 0\n",
      "episode: 361/400, score: 0.0, cars: 0\n",
      "episode: 362/400, score: 0.0, cars: 0\n",
      "episode: 363/400, score: 0.0, cars: 0\n",
      "episode: 364/400, score: 0.0, cars: 0\n",
      "episode: 365/400, score: 0.0, cars: 0\n",
      "episode: 366/400, score: 0.0, cars: 0\n",
      "episode: 367/400, score: 0.0, cars: 0\n",
      "episode: 368/400, score: 8.0, cars: 0\n",
      "episode: 369/400, score: 0.0, cars: 0\n",
      "episode: 370/400, score: 0.0, cars: 0\n",
      "episode: 371/400, score: 6.0, cars: 0\n",
      "episode: 372/400, score: 32.0, cars: 0\n",
      "episode: 373/400, score: 0.0, cars: 0\n",
      "episode: 374/400, score: 0.0, cars: 0\n",
      "episode: 375/400, score: 0.0, cars: 0\n",
      "episode: 376/400, score: 16.0, cars: 0\n",
      "episode: 377/400, score: 0.0, cars: 0\n",
      "episode: 378/400, score: 19.0, cars: 0\n",
      "episode: 379/400, score: 23.0, cars: 0\n",
      "episode: 380/400, score: 0.0, cars: 0\n",
      "episode: 381/400, score: 0.0, cars: 0\n",
      "episode: 382/400, score: 32.0, cars: 0\n",
      "episode: 383/400, score: 0.0, cars: 0\n",
      "episode: 384/400, score: 14.0, cars: 0\n",
      "episode: 385/400, score: 0.0, cars: 0\n",
      "episode: 386/400, score: 0.0, cars: 0\n",
      "episode: 387/400, score: 0.0, cars: 0\n",
      "episode: 388/400, score: 41.0, cars: 0\n",
      "episode: 389/400, score: 15.0, cars: 0\n",
      "episode: 390/400, score: 0.0, cars: 0\n",
      "episode: 391/400, score: 0.0, cars: 0\n",
      "episode: 392/400, score: 18.0, cars: 0\n",
      "episode: 393/400, score: 0.0, cars: 0\n",
      "episode: 394/400, score: 0.0, cars: 0\n",
      "episode: 395/400, score: 0.0, cars: 0\n",
      "episode: 396/400, score: 0.0, cars: 0\n",
      "episode: 397/400, score: 0.0, cars: 0\n",
      "episode: 398/400, score: 19.0, cars: 0\n",
      "episode: 399/400, score: 0.0, cars: 0\n",
      "episode: 400/400, score: 10.0, cars: 0\n"
     ]
    }
   ],
   "source": [
    "#realizacja ze stackowaniem klatek\n",
    "if __name__ == \"__main__\":\n",
    "    # initialize gym environment and the agent\n",
    "    env = gym.make('Enduro-v0')\n",
    "    agent = DuelingAgent(env)\n",
    "    episodes=400\n",
    "    #stack_size = 4 # We stack 4 frames\n",
    "    # Initialize deque with zero-images one array for each image\n",
    "    #stacked_frames  =  deque([np.zeros((105,160), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
    "    # Iterate the game\n",
    "    for e in range(episodes):\n",
    "        score = 0\n",
    "        # reset state in the beginning of each game\n",
    "        state = preprocess_frame(env.reset())\n",
    "        cars=0\n",
    "        max_cars=0\n",
    "        for time_t in range(5000):           \n",
    "            action = agent.act(state)\n",
    "            # Advance the game to the next frame based on the action.\n",
    "            # Reward is 1 for every frame the pole survived\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            score=score+reward\n",
    "            next_state = preprocess_frame(next_state)\n",
    "            # Remember the previous state, action, reward, and done\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            # make next_state the new current state for the next frame.\n",
    "            state = next_state\n",
    "            # done becomes True when the game ends\n",
    "            # ex) The agent drops the pole\n",
    "            if done or time_t==4999:\n",
    "                # print the score and break out of the loop\n",
    "                print(\"episode: {}/{}, score: {}, cars: {}\"\n",
    "                      .format(e+1, episodes, score, cars))\n",
    "                break\n",
    "        # train the agent with the experience of the episode\n",
    "        agent.replay(1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zapisujemy uzyskany model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dobry = agent.model.to_json()\n",
    "with open(\"model_nowy_pre.json\", \"w\") as json_file:\n",
    "    json_file.write(model_dobry)\n",
    "agent.model.save_weights(\"model_nowy_pre.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I wczytujemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/arkadiusz/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "json_file = open('model_nowy_pre.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights(\"model_nowy_pre.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testujemy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testuj(model, stack=False):\n",
    "    env = gym.make('Enduro-v0')\n",
    "    scores = []\n",
    "    choices = []\n",
    "    for each_game in range(5):\n",
    "        score = 0\n",
    "        prev_obs = []\n",
    "        stack_size = 4 \n",
    "        stacked_frames  =  deque([np.zeros((105,160), dtype=np.int) for i in range(4)], maxlen=4)\n",
    "        state = env.reset()\n",
    "        for step_index in range(5000):\n",
    "            env.render()\n",
    "            if stack:\n",
    "                if step_index==0:\n",
    "                    state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "                    action=random.randint(1,3)\n",
    "                else:\n",
    "                    action = np.argmax(model.predict(prev_obs))\n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "                prev_obs, stacked_frames = stack_frames(stacked_frames, new_observation, False)\n",
    "                prev_obs=prev_obs.reshape(1, *prev_obs.shape)\n",
    "                score+=reward\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "            else:\n",
    "                if step_index==0:\n",
    "                    action=random.randint(1,3)\n",
    "                else:\n",
    "                    action = np.argmax(model.predict(prev_obs))\n",
    "                choices.append(action)\n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "                prev_obs = preprocess_frame(new_observation)\n",
    "                prev_obs=prev_obs.reshape(1, *prev_obs.shape)\n",
    "                score+=reward\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "        env.reset()\n",
    "        scores.append(score)\n",
    "    env.close()\n",
    "\n",
    "    print(scores)\n",
    "    print('Average Score:', sum(scores)/len(scores))\n",
    "    #print('choice 1:{}  choice 2:{}'.format(choices.count(1)/len(choices),choices.count(2)/len(choices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.0, 22.0, 34.0, 16.0, 16.0]\n",
      "Average Score: 22.0\n"
     ]
    }
   ],
   "source": [
    "testuj(agent.model, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
